{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6376134,"sourceType":"datasetVersion","datasetId":3674161},{"sourceId":7674705,"sourceType":"datasetVersion","datasetId":4476797}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch_geometric > /dev/null\n# !pip install qiskit > /dev/null\n# !pip install qiskit_aer > /dev/null\n# !pip install pennylane\n# !pip install qiskit-aer-gpu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:57:19.905254Z","iopub.execute_input":"2025-04-23T12:57:19.905519Z","iopub.status.idle":"2025-04-23T12:57:26.377941Z","shell.execute_reply.started":"2025-04-23T12:57:19.905497Z","shell.execute_reply":"2025-04-23T12:57:26.376993Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# !pip install --upgrade \"pennylane-lightning[gpu]\"  # pulls pennylane + plugin\n# !pip install --upgrade custatevec-cu12           # CUDAÂ 11 build of cuQuantum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:57:26.379150Z","iopub.execute_input":"2025-04-23T12:57:26.379488Z","iopub.status.idle":"2025-04-23T12:57:26.383676Z","shell.execute_reply.started":"2025-04-23T12:57:26.379451Z","shell.execute_reply":"2025-04-23T12:57:26.382979Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import concurrent.futures\n\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nimport os\n\nimport pandas as pd\n\n# import pennylane as qml\n# from pennylane import DeviceError\n\n\n\n# from qiskit import QuantumCircuit\n# from qiskit.circuit import ParameterVector\n# from qiskit.quantum_info import SparsePauliOp, Statevector\n# from qiskit_aer import AerSimulator\n# from qiskit_aer.primitives import EstimatorV2\n\nfrom sklearn.metrics import average_precision_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport torch\nfrom torch.func import vmap  \nimport torch.nn as nn\nfrom torch.nn import Linear\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\n\nimport torch_geometric\nfrom torch_geometric.datasets import JODIEDataset\nfrom torch_geometric.loader import TemporalDataLoader\nfrom torch_geometric.nn.conv import TransformerConv\nfrom torch_geometric.nn.models import tgn\nfrom torch_geometric.nn.models.tgn import TGNMemory, IdentityMessage, LastNeighborLoader, LastAggregator\n\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:57:26.385516Z","iopub.execute_input":"2025-04-23T12:57:26.385751Z","iopub.status.idle":"2025-04-23T12:57:38.478039Z","shell.execute_reply.started":"2025-04-23T12:57:26.385730Z","shell.execute_reply":"2025-04-23T12:57:38.477169Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# data = pd.read_csv(\"/kaggle/input/dataset-invade/dataset_invade.csv\")\n# features = data.drop(columns=['attack'])\n# target = data['attack'].apply(lambda x: 1 if x == 'Yes' else 0)\n\n# # Convert categorical columns to one-hot encoded columns\n# non_numeric_columns = features.select_dtypes(include=['object']).columns\n# features = pd.get_dummies(features, columns=non_numeric_columns)\n\n# # Convert boolean columns to integers\n# for col in features.select_dtypes(include=['bool']).columns:\n#     features[col] = features[col].astype(int)\n\n# scaler = StandardScaler()\n# features = scaler.fit_transform(features)\n\n# # Convert to PyTorch tensors\n# X_tensor = torch.tensor(features, dtype=torch.float32)\n# y_tensor = torch.tensor(target, dtype=torch.float32)\n\n# # Display the shapes of the tensors\n# print(X_tensor.shape, y_tensor.shape)\n\n# # Split the data into training, validation, and test sets\n# X_train, X_temp, y_train, y_temp = train_test_split(X_tensor, y_tensor, test_size=0.3, random_state=42)\n# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# # Create DataLoader for each dataset\n# batch_size = 64\n\n# train_dataset = TensorDataset(X_train, y_train)\n# val_dataset = TensorDataset(X_val, y_val)\n# test_dataset = TensorDataset(X_test, y_test)\n\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:57:38.478930Z","iopub.execute_input":"2025-04-23T12:57:38.479341Z","iopub.status.idle":"2025-04-23T12:57:38.484398Z","shell.execute_reply.started":"2025-04-23T12:57:38.479318Z","shell.execute_reply":"2025-04-23T12:57:38.483347Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# This one i think\n","metadata":{}},{"cell_type":"code","source":"# class SineLayer(torch.nn.Module):\n#     def __init__(self, input_dim, output_dim):\n#         super(QuantumLayer, self).__init__()\n#         self.linear = torch.nn.Linear(input_dim, output_dim)\n\n#     def forward(self, x):\n#         x = self.linear(x)\n#         x = torch.sin(x)  # Apply a quantum-inspired non-linearity\n#         return x\n\n# class SineNN(torch.nn.Module):\n#     def __init__(self, input_dim):\n#         super(QuantumNN, self).__init__()\n#         self.sine_layer1 = SineLayer(input_dim, 128)\n#         self.sine_layer2 = SineLayer(128, 64)\n#         self.fc = torch.nn.Linear(64, 1)\n#         self.dropout = torch.nn.Dropout(0.5)\n\n#     def forward(self, x):\n#         x = torch.relu(self.sine_layer1(x))\n#         x = self.dropout(x)\n#         x = torch.relu(self.sine_layer2(x))\n#         x = self.dropout(x)\n#         x = torch.sigmoid(self.fc(x))\n#         return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:57:38.485486Z","iopub.execute_input":"2025-04-23T12:57:38.485858Z","iopub.status.idle":"2025-04-23T12:57:38.521268Z","shell.execute_reply.started":"2025-04-23T12:57:38.485800Z","shell.execute_reply":"2025-04-23T12:57:38.520368Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# class SimplerFNN(torch.nn.Module):\n#     def __init__(self, input_dim):\n#         super().__init__()\n#         self.model = torch.nn.Sequential(\n#             torch.nn.Linear(input_dim, 64),\n#             torch.nn.ReLU(),\n#             torch.nn.Linear(64, 32),\n#             torch.nn.ReLU(),\n#             torch.nn.Linear(32, 1),\n#             torch.nn.Sigmoid()\n#         )\n\n#     def forward(self, x):\n#         return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:57:38.522204Z","iopub.execute_input":"2025-04-23T12:57:38.522577Z","iopub.status.idle":"2025-04-23T12:57:38.545326Z","shell.execute_reply.started":"2025-04-23T12:57:38.522533Z","shell.execute_reply":"2025-04-23T12:57:38.544185Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# # Training function\n# def train_model(model, optimizer, loader):\n#     model.train()\n#     running_loss = 0.0\n#     for inputs, labels in loader:\n#         optimizer.zero_grad()\n#         outputs = model(inputs)\n#         loss = criterion(outputs.squeeze(), labels)\n#         loss.backward()\n#         optimizer.step()\n#         running_loss += loss.item() * inputs.size(0)\n#     return running_loss / len(loader.dataset)\n\n# # Evaluation function\n# def evaluate_model(model, optimizer, loader):\n#     model.eval()\n#     running_loss = 0.0\n#     all_outputs = []\n#     all_labels = []\n#     with torch.no_grad():\n#         for inputs, labels in loader:\n#             outputs = model(inputs)\n#             loss = criterion(outputs.squeeze(), labels)\n#             running_loss += loss.item() * inputs.size(0)\n#             all_outputs.append(outputs.squeeze().cpu())\n#             all_labels.append(labels.cpu())\n#     all_outputs = torch.cat(all_outputs)\n#     all_labels = torch.cat(all_labels)\n#     return running_loss / len(loader.dataset), all_outputs, all_labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:57:38.546239Z","iopub.execute_input":"2025-04-23T12:57:38.546490Z","iopub.status.idle":"2025-04-23T12:57:38.563266Z","shell.execute_reply.started":"2025-04-23T12:57:38.546467Z","shell.execute_reply":"2025-04-23T12:57:38.562268Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# def run_it(num_epochs, model, optimizer, train_loader, val_loader, test_loader):\n#     for epoch in range(1, num_epochs + 1):\n#         if epoch > 30:\n#             if sum(val_losses[-20:-10]) < sum(val_losses[-10:]):\n#                 print(\"Early Stopping\")\n#                 break\n#         train_loss = train_model(model, optimizer, train_loader)\n#         val_loss, val_outputs, val_labels = evaluate_model(model, optimizer, val_loader)\n#         test_loss, test_outputs, test_labels = evaluate_model(model, optimizer, test_loader)\n    \n#         val_ap = average_precision_score(val_labels, val_outputs)\n#         val_auc = roc_auc_score(val_labels, val_outputs)\n#         test_ap = average_precision_score(test_labels, test_outputs)\n#         test_auc = roc_auc_score(test_labels, test_outputs)\n    \n#         train_losses.append(train_loss)\n#         val_losses.append(val_loss)\n#         val_aps.append(val_ap)\n#         val_aucs.append(val_auc)\n#         test_aps.append(test_ap)\n#         test_aucs.append(test_auc)\n    \n#         print(f'Epoch: {epoch:02d}, Loss: {train_loss:.4f} Val AP: {val_ap:.4f}, Val AUC: {val_auc:.4f} Test AP: {test_ap:.4f}, Test AUC: {test_auc:.4f}')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:57:38.564018Z","iopub.execute_input":"2025-04-23T12:57:38.564373Z","iopub.status.idle":"2025-04-23T12:57:38.587085Z","shell.execute_reply.started":"2025-04-23T12:57:38.564324Z","shell.execute_reply":"2025-04-23T12:57:38.586156Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# input_dim = X_tensor.shape[1]\n# criterion = torch.nn.BCELoss()\n# model = SimplerFNN(input_dim)\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n\n\n# # Training loop\n# num_epochs = 200\n# train_losses = []\n# val_losses = []\n# val_aps = []\n# val_aucs = []\n# test_aps = []\n# test_aucs = []\n\n# run_it(num_epochs, model, optimizer, train_loader, val_loader, test_loader)\n\n","metadata":{"id":"srkmVbXYLE-a","outputId":"798bdbc2-5c08-4699-c43e-a5df92c42ee1","trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:57:38.590708Z","iopub.execute_input":"2025-04-23T12:57:38.591670Z","iopub.status.idle":"2025-04-23T12:57:38.609539Z","shell.execute_reply.started":"2025-04-23T12:57:38.591632Z","shell.execute_reply":"2025-04-23T12:57:38.608286Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Output from cell above\n\n```Epoch: 01, Loss: 0.0933 Val AP: 0.9980, Val AUC: 0.9980 Test AP: 0.9978, Test AUC: 0.9982\nEpoch: 02, Loss: 0.0498 Val AP: 0.9984, Val AUC: 0.9984 Test AP: 0.9984, Test AUC: 0.9986\nEpoch: 03, Loss: 0.0444 Val AP: 0.9986, Val AUC: 0.9987 Test AP: 0.9985, Test AUC: 0.9987\nEpoch: 04, Loss: 0.0419 Val AP: 0.9987, Val AUC: 0.9987 Test AP: 0.9987, Test AUC: 0.9988\nEpoch: 05, Loss: 0.0397 Val AP: 0.9988, Val AUC: 0.9989 Test AP: 0.9988, Test AUC: 0.9989\nEpoch: 06, Loss: 0.0386 Val AP: 0.9987, Val AUC: 0.9988 Test AP: 0.9987, Test AUC: 0.9989\nEpoch: 07, Loss: 0.0372 Val AP: 0.9987, Val AUC: 0.9989 Test AP: 0.9988, Test AUC: 0.9990\nEpoch: 08, Loss: 0.0363 Val AP: 0.9988, Val AUC: 0.9990 Test AP: 0.9988, Test AUC: 0.9990\nEpoch: 09, Loss: 0.0352 Val AP: 0.9988, Val AUC: 0.9990 Test AP: 0.9989, Test AUC: 0.9991\nEpoch: 10, Loss: 0.0340 Val AP: 0.9988, Val AUC: 0.9989 Test AP: 0.9989, Test AUC: 0.9990\nEpoch: 11, Loss: 0.0338 Val AP: 0.9988, Val AUC: 0.9989 Test AP: 0.9988, Test AUC: 0.9990\nEpoch: 12, Loss: 0.0325 Val AP: 0.9990, Val AUC: 0.9991 Test AP: 0.9989, Test AUC: 0.9991\nEpoch: 13, Loss: 0.0339 Val AP: 0.9990, Val AUC: 0.9991 Test AP: 0.9989, Test AUC: 0.9991\nEpoch: 14, Loss: 0.0347 Val AP: 0.9991, Val AUC: 0.9991 Test AP: 0.9989, Test AUC: 0.9992\nEpoch: 15, Loss: 0.0340 Val AP: 0.9990, Val AUC: 0.9991 Test AP: 0.9989, Test AUC: 0.9991\nEpoch: 16, Loss: 0.0335 Val AP: 0.9991, Val AUC: 0.9992 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 17, Loss: 0.0332 Val AP: 0.9991, Val AUC: 0.9991 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 18, Loss: 0.0327 Val AP: 0.9990, Val AUC: 0.9991 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 19, Loss: 0.0332 Val AP: 0.9990, Val AUC: 0.9991 Test AP: 0.9989, Test AUC: 0.9992\nEpoch: 20, Loss: 0.0328 Val AP: 0.9990, Val AUC: 0.9991 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 21, Loss: 0.0318 Val AP: 0.9990, Val AUC: 0.9991 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 22, Loss: 0.0313 Val AP: 0.9991, Val AUC: 0.9992 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 23, Loss: 0.0308 Val AP: 0.9991, Val AUC: 0.9991 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 24, Loss: 0.0303 Val AP: 0.9991, Val AUC: 0.9991 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 25, Loss: 0.0299 Val AP: 0.9991, Val AUC: 0.9991 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 26, Loss: 0.0298 Val AP: 0.9991, Val AUC: 0.9992 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 27, Loss: 0.0293 Val AP: 0.9991, Val AUC: 0.9991 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 28, Loss: 0.0294 Val AP: 0.9991, Val AUC: 0.9992 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 29, Loss: 0.0286 Val AP: 0.9992, Val AUC: 0.9992 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 30, Loss: 0.0289 Val AP: 0.9991, Val AUC: 0.9991 Test AP: 0.9989, Test AUC: 0.9991\nEpoch: 31, Loss: 0.0284 Val AP: 0.9992, Val AUC: 0.9992 Test AP: 0.9991, Test AUC: 0.9993\nEpoch: 32, Loss: 0.0285 Val AP: 0.9991, Val AUC: 0.9991 Test AP: 0.9991, Test AUC: 0.9993\nEpoch: 33, Loss: 0.0281 Val AP: 0.9989, Val AUC: 0.9991 Test AP: 0.9990, Test AUC: 0.9992\nEpoch: 34, Loss: 0.0283 Val AP: 0.9991, Val AUC: 0.9992 Test AP: 0.9991, Test AUC: 0.9993\nEpoch: 35, Loss: 0.0276 Val AP: 0.9990, Val AUC: 0.9991 Test AP: 0.9991, Test AUC: 0.9993\nEpoch: 36, Loss: 0.0274 Val AP: 0.9990, Val AUC: 0.9991 Test AP: 0.9991, Test AUC: 0.9992\nEpoch: 37, Loss: 0.0270 Val AP: 0.9990, Val AUC: 0.9991 Test AP: 0.9991, Test AUC: 0.9993\nEarly Stopping```","metadata":{}},{"cell_type":"markdown","source":"# Multiclass","metadata":{}},{"cell_type":"code","source":"data_path = \"/kaggle/input/network-intrusion-dataset/\"\ndata_files = os.listdir(\"/kaggle/input/network-intrusion-dataset/\")\n\n# Load and concatenate all CSV files into a single DataFrame\ndata_list = []\nfor file in data_files:\n    if file.endswith('.csv'):\n        file_path = os.path.join(data_path, file)\n        data_list.append(pd.read_csv(file_path))\n\ndata = pd.concat(data_list, ignore_index=True)\nprint(\"Data loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:57:38.610597Z","iopub.execute_input":"2025-04-23T12:57:38.610964Z","iopub.status.idle":"2025-04-23T12:58:11.145891Z","shell.execute_reply.started":"2025-04-23T12:57:38.610932Z","shell.execute_reply":"2025-04-23T12:58:11.144894Z"}},"outputs":[{"name":"stdout","text":"Data loaded successfully!\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Strip any leading/trailing whitespace characters from the column names\ndata.columns = data.columns.str.strip()\n\n# Drop columns that won't be used for training (like non-numeric features)\ncolumns_to_drop = ['Flow ID', 'Source IP', 'Source Port', 'Destination IP', 'Timestamp']\ncolumns_to_drop = [col.strip() for col in columns_to_drop if col.strip() in data.columns]\ndata = data.drop(columns=columns_to_drop)\n\n# Convert categorical columns to one-hot encoded columns\nnon_numeric_columns = data.select_dtypes(include=['object']).columns\ndata = pd.get_dummies(data, columns=non_numeric_columns)\n\n# Handle infinity or extremely large values\ndata = data.replace([np.inf, -np.inf], np.nan).dropna()\n\n# Identify the target columns\nlabel_columns = [col for col in data.columns if 'Label_' in col]\n\n# Combine one-hot encoded label columns into a single target column\ndata['Label'] = data[label_columns].idxmax(axis=1).apply(lambda x: x.split('_')[1])\n\n# Map the labels to unique integers for multi-class classification\nlabel_mapping = {label: idx for idx, label in enumerate(data['Label'].unique())}\ndata['Label'] = data['Label'].map(label_mapping)\n\n# Drop the individual label columns\ndata = data.drop(columns=label_columns)\n\n# Separate features and target\nfeatures = data.drop(columns=['Label'])\ntarget = data['Label']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:11.146729Z","iopub.execute_input":"2025-04-23T12:58:11.147043Z","iopub.status.idle":"2025-04-23T12:58:21.690679Z","shell.execute_reply.started":"2025-04-23T12:58:11.147013Z","shell.execute_reply":"2025-04-23T12:58:21.689654Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"data['Label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T13:20:10.936225Z","iopub.execute_input":"2025-04-23T13:20:10.936650Z","iopub.status.idle":"2025-04-23T13:20:10.970315Z","shell.execute_reply.started":"2025-04-23T13:20:10.936616Z","shell.execute_reply":"2025-04-23T13:20:10.969366Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Label\n0     2271320\n12     230124\n1      158804\n7      128025\n13      10293\n5        7935\n6        5897\n10       5796\n11       5499\n8        1956\n2        1507\n3         652\n9          36\n4          21\n14         11\nName: count, dtype: int64"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T13:20:53.338711Z","iopub.execute_input":"2025-04-23T13:20:53.339102Z","iopub.status.idle":"2025-04-23T13:20:53.356019Z","shell.execute_reply.started":"2025-04-23T13:20:53.339073Z","shell.execute_reply":"2025-04-23T13:20:53.354556Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"tensor([ 0,  0,  0,  ..., 12,  0, 12])"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# Feature scaling\nscaler = StandardScaler()\nfeatures = scaler.fit_transform(features)\n\n# Convert to PyTorch tensors\nX_tensor = torch.tensor(features, dtype=torch.float32)\ny_tensor = torch.tensor(target.values, dtype=torch.long)\n\n# Split the data into training, validation, and test sets\nX_train, X_temp, y_train, y_temp = train_test_split(X_tensor, y_tensor, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\n# Create DataLoader for each dataset\nbatch_size = 1024\n\ntrain_dataset = TensorDataset(X_train, y_train)\nval_dataset = TensorDataset(X_val, y_val)\ntest_dataset = TensorDataset(X_test, y_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=4, pin_memory=True)\n# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1024, shuffle=True, num_workers=4, pin_memory=True)\n\n# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=1024, shuffle=True, num_workers=4, pin_memory=True)\n\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:21.691571Z","iopub.execute_input":"2025-04-23T12:58:21.691873Z","iopub.status.idle":"2025-04-23T12:58:30.728937Z","shell.execute_reply.started":"2025-04-23T12:58:21.691843Z","shell.execute_reply":"2025-04-23T12:58:30.728087Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"targ_val = y_train\nvals = torch.unique(targ_val, return_counts=True)[0]\ncnts = torch.unique(targ_val, return_counts=True)[1]\nfor val, cnt in zip(vals, cnts):\n    print(f'{val}: {cnt}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T13:26:50.049378Z","iopub.execute_input":"2025-04-23T13:26:50.049698Z","iopub.status.idle":"2025-04-23T13:26:50.180950Z","shell.execute_reply.started":"2025-04-23T13:26:50.049676Z","shell.execute_reply":"2025-04-23T13:26:50.179975Z"}},"outputs":[{"name":"stdout","text":"0: 1590392\n1: 111054\n2: 1099\n3: 427\n4: 16\n5: 5574\n6: 4109\n7: 89542\n8: 1400\n9: 28\n10: 4039\n11: 3841\n12: 160857\n13: 7130\n14: 5\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"targ_val = y_val\nvals = torch.unique(targ_val, return_counts=True)[0]\ncnts = torch.unique(targ_val, return_counts=True)[1]\nfor val, cnt in zip(vals, cnts):\n    print(f'{val}: {cnt}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T13:27:09.808001Z","iopub.execute_input":"2025-04-23T13:27:09.808333Z","iopub.status.idle":"2025-04-23T13:27:09.838807Z","shell.execute_reply.started":"2025-04-23T13:27:09.808307Z","shell.execute_reply":"2025-04-23T13:27:09.837864Z"}},"outputs":[{"name":"stdout","text":"0: 340317\n1: 24056\n2: 190\n3: 97\n4: 2\n5: 1185\n6: 907\n7: 19239\n8: 278\n9: 4\n10: 881\n11: 833\n12: 34660\n13: 1528\n14: 4\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"targ_val = y_test\nvals = torch.unique(targ_val, return_counts=True)[0]\ncnts = torch.unique(targ_val, return_counts=True)[1]\nfor val, cnt in zip(vals, cnts):\n    print(f'{val}: {cnt}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T13:27:24.416355Z","iopub.execute_input":"2025-04-23T13:27:24.416934Z","iopub.status.idle":"2025-04-23T13:27:24.461146Z","shell.execute_reply.started":"2025-04-23T13:27:24.416894Z","shell.execute_reply":"2025-04-23T13:27:24.460137Z"}},"outputs":[{"name":"stdout","text":"0: 340611\n1: 23694\n2: 218\n3: 128\n4: 3\n5: 1176\n6: 881\n7: 19244\n8: 278\n9: 4\n10: 876\n11: 825\n12: 34607\n13: 1635\n14: 2\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"# Define the model, I changed the shape of the network, why scale from 79 features to a 128 wide model?\nclass SimpleNN(torch.nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.model = torch.nn.Sequential(\n            torch.nn.Linear(input_dim, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64, 32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32, num_classes),\n            # torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n# Training function\nscaler = torch.amp.GradScaler('cuda')\n\ndef train_model(loader):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in tqdm(loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        with torch.amp.autocast('cuda'):\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item() * inputs.size(0)\n        # print(\"OUTPUT SHAPE:\", outputs.shape)\n        # print(\"LABEL SHAPE:\", labels.shape)\n    return running_loss / len(loader.dataset)\n\n\n# Evaluation function\ndef evaluate_model(loader):\n    model.eval()\n    running_loss = 0.0\n    all_outputs = []\n    all_labels = []\n\n    with torch.no_grad():\n        for inputs, labels in loader:\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)  # raw logits\n            loss = criterion(outputs, labels)\n            running_loss += loss.item() * inputs.size(0)\n\n            probs = F.softmax(outputs, dim=1)  # convert to probabilities\n            all_outputs.append(probs.cpu())\n            all_labels.append(labels.cpu())\n\n    all_outputs = torch.cat(all_outputs)\n    all_labels = torch.cat(all_labels)\n    return running_loss / len(loader.dataset), all_outputs, all_labels\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.730020Z","iopub.execute_input":"2025-04-23T12:58:30.730416Z","iopub.status.idle":"2025-04-23T12:58:30.744289Z","shell.execute_reply.started":"2025-04-23T12:58:30.730390Z","shell.execute_reply":"2025-04-23T12:58:30.743351Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"\ninput_dim = X_tensor.shape[1]\nNUM_CLASSES = len(label_mapping)\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = SimpleNN(input_dim=input_dim, num_classes=NUM_CLASSES).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.745233Z","iopub.execute_input":"2025-04-23T12:58:30.746096Z","iopub.status.idle":"2025-04-23T12:58:30.790512Z","shell.execute_reply.started":"2025-04-23T12:58:30.746058Z","shell.execute_reply":"2025-04-23T12:58:30.789454Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Quantum Tests\n","metadata":{}},{"cell_type":"code","source":"# class HybridNN(nn.Module):\n#     def __init__(self, input_dim, num_classes):\n#         super().__init__()\n\n#         self.scale = nn.Parameter(torch.ones(8) * np.pi)\n\n#         self.classical = nn.Sequential(\n#             nn.Linear(input_dim, 64),\n#             nn.ReLU(),\n#             nn.Linear(64, 32),\n#             nn.ReLU(),\n#             nn.Linear(32, 8),\n#             nn.ReLU()\n#         )\n\n#         self.output_layer = nn.Sequential(\n#             nn.Linear(8, 64),\n#             nn.ReLU(),\n#             nn.Linear(64, 32),\n#             nn.ReLU(),\n#             nn.Linear(32, num_classes)\n#         )\n\n#         self.simulator = AerSimulator(method='statevector', device = 'CPU')\n#         self.estimator = EstimatorV2.from_backend(self.simulator)\n\n#         # Create parameterized circuit (once)\n#         self.params = ParameterVector(\"Î¸\", 8)\n#         self.qc_template = QuantumCircuit(8)\n#         for i in range(8):\n#             self.qc_template.ry(self.params[i], i)\n#         for i in range(8):\n#             self.qc_template.cz(i, (i + 1) % 8)\n\n#         # Observables: Z_i for each qubit\n#         self.observables = [\n#             SparsePauliOp.from_list([(f\"{'I'*i}Z{'I'*(7-i)}\", 1.0)])\n#             for i in range(8)\n#         ]\n\n#     def quantum_layer(self, x_batch):\n#         \"\"\"\n#         x_batch: Tensor of shape (batch_size, 8)\n#         Returns: Tensor of shape (batch_size, 8) with Z expectation values\n#         \"\"\"\n#         # Convert scaled inputs to list of NumPy arrays\n#         batch_scaled = (self.scale * x_batch).detach().cpu().numpy()\n    \n#         # Map each input to param dict (ParameterVector already set in init)\n#         param_list = [dict(zip(self.params, row)) for row in batch_scaled]\n    \n#         # Create one circuit (will be copied internally)\n#         jobs = [(self.qc_template.assign_parameters(p, inplace=False), self.observables) for p in param_list]\n    \n#         # Run batch job\n#         result = self.estimator.run(jobs).result()\n    \n#         # Stack results back into tensor\n#         evs = [torch.tensor(r.data.evs, dtype=torch.float32) for r in result]\n#         return torch.stack(evs).to(x_batch.device)\n\n#     def forward(self, x):\n#         x = self.classical(x)             # shape: (batch_size, 8)\n#         x = self.quantum_layer(x)         # shape: (batch_size, 8)\n#         x = self.output_layer(x)          # shape: (batch_size, num_classes)\n#         return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.791653Z","iopub.execute_input":"2025-04-23T12:58:30.791959Z","iopub.status.idle":"2025-04-23T12:58:30.798505Z","shell.execute_reply.started":"2025-04-23T12:58:30.791935Z","shell.execute_reply":"2025-04-23T12:58:30.797558Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# # Constants\n# scale = np.array([np.pi for _ in range(8)])\n# simulator = AerSimulator(method='statevector')\n# estimator = EstimatorV2.from_backend(simulator)\n\n# # Individual observables: Z_i for i=0..7\n# observables = [SparsePauliOp.from_list([(f\"{'I'*i}Z{'I'*(7-i)}\", 1.0)]) for i in range(8)]\n\n# # Input: shape (8, 1024)\n# x = np.random.rand(8, 1024)\n# scaled = scale[:, None] * x\n\n# results = []\n\n# for col in range(scaled.shape[1]):\n#     theta_sample = scaled[:, col]\n\n#     qc = QuantumCircuit(8)\n#     for q in range(8):\n#         qc.ry(theta_sample[q], q)\n#     for q in range(8):\n#         qc.cz(q, (q + 1) % 8)\n\n#     job = estimator.run([(qc, observables)])\n#     result = job.result()\n#     results.append(result[0].data.evs)\n\n# # Result: shape (1024, 8)\n# result_array = np.array(results)\n# print(result_array.shape)\n# print(\"First result:\", result_array[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.799524Z","iopub.execute_input":"2025-04-23T12:58:30.800092Z","iopub.status.idle":"2025-04-23T12:58:30.821331Z","shell.execute_reply.started":"2025-04-23T12:58:30.800066Z","shell.execute_reply":"2025-04-23T12:58:30.820266Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# class HybridNN(nn.Module):\n#     def __init__(self, input_dim, num_classes, n_qubits=8):\n#         super().__init__()\n#         self.n_qubits = n_qubits\n#         self.scale = nn.Parameter(torch.ones(n_qubits) * np.pi)\n\n#         self.classical = nn.Sequential(\n#             nn.Linear(input_dim, 64),\n#             nn.ReLU(),\n#             nn.Linear(64, 32),\n#             nn.ReLU(),\n#             nn.Linear(32, n_qubits),\n#             nn.ReLU()\n#         )\n\n#         self.output_layer = nn.Sequential(\n#             nn.Linear(n_qubits, 16),\n#             nn.ReLU(),\n#             nn.Linear(16, num_classes)\n#         )\n\n#         # Precompute Z_i matrices for matrix inner product method\n#         self.register_buffer(\"Z_ops\", self._precompute_z_ops(n_qubits))\n\n#     def _precompute_z_ops(self, n_qubits):\n#         Z = torch.tensor([[1, 0], [0, -1]], dtype=torch.complex64)\n#         I = torch.eye(2, dtype=torch.complex64)\n#         Z_ops = []\n\n#         for i in range(n_qubits):\n#             op = None\n#             for j in range(n_qubits):\n#                 mat = Z if j == i else I\n#                 op = mat if op is None else torch.kron(op, mat)\n#             Z_ops.append(op)\n#         return torch.stack(Z_ops)  # shape: (n_qubits, 2**n, 2**n)\n\n#     def quantum_layer(self, x_batch):\n#         \"\"\"\n#         x_batch: Tensor of shape (batch_size, n_qubits)\n#         Returns: Tensor of shape (batch_size, n_qubits)\n#         \"\"\"\n#         device = x_batch.device\n#         batch_size, n_qubits = x_batch.shape\n#         results = []\n    \n#         for b in range(batch_size):\n#             x = x_batch[b].detach().cpu().numpy() * self.scale.detach().cpu().numpy()\n    \n#             # Build and simulate circuit\n#             qc = QuantumCircuit(n_qubits)\n#             for i in range(n_qubits):\n#                 qc.ry(x[i], i)\n#             for i in range(n_qubits):\n#                 qc.cz(i, (i + 1) % n_qubits)\n    \n#             psi_np = Statevector.from_instruction(qc).data\n#             psi = torch.tensor(psi_np, dtype=torch.complex64, device=device)  # shape: (2**n,)\n    \n#             # Compute expectation values âŸ¨Ïˆ|Z_i|ÏˆâŸ©\n#             evs = torch.einsum('i,bij,j->b', psi.conj(), self.Z_ops.to(device), psi)\n#             results.append(evs.real)\n    \n#         return torch.stack(results)  # (batch_size, n_qubits)\n\n\n\n#     def forward(self, x):\n#         x = self.classical(x)             # shape: (batch_size, 8)\n#         x = self.quantum_layer(x)         # shape: (batch_size, 8)\n#         x = self.output_layer(x)          # shape: (batch_size, num_classes)\n#         return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.822518Z","iopub.execute_input":"2025-04-23T12:58:30.822839Z","iopub.status.idle":"2025-04-23T12:58:30.839503Z","shell.execute_reply.started":"2025-04-23T12:58:30.822784Z","shell.execute_reply":"2025-04-23T12:58:30.838561Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"# class TorchQuantumLayer(nn.Module):\n#     def __init__(self):\n#         super().__init__()\n#         self.num_qubits = 8\n#         self.scale = nn.Parameter(torch.ones(self.num_qubits) * torch.pi)\n\n#         # Define Pauli-Z operators (IâŠ—IâŠ—...âŠ—ZâŠ—...âŠ—I) in matrix form\n#         Z = torch.tensor([[1., 0.], [0., -1.]], dtype=torch.float32)\n#         I = torch.eye(2, dtype=torch.float32)\n\n#         self.paulis = []\n#         for i in range(self.num_qubits):\n#             ops = [I] * self.num_qubits\n#             ops[i] = Z\n#             pauli = ops[0]\n#             for op in ops[1:]:\n#                 pauli = torch.kron(pauli, op)\n#             self.paulis.append(pauli)\n\n#         self.paulis = torch.stack(self.paulis).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#     def forward(self, x):\n#         \"\"\"\n#         x: (batch_size, 8)\n#         Returns: (batch_size, 8) expectation values\n#         \"\"\"\n#         device = x.device\n#         batch_size = x.shape[0]\n#         angles = x * self.scale  # shape: (batch_size, 8)\n\n#         # Build quantum states for each sample: apply Ry(Î¸) to |0âŸ©^âŠ—n\n#         zero_state = torch.tensor([1.0, 0.0], dtype=torch.float32, device=device)\n#         full_zero = zero_state\n#         for _ in range(self.num_qubits - 1):\n#             full_zero = torch.kron(full_zero, zero_state)  # shape: (2^n,)\n\n#         state_batch = []\n#         for sample in angles:\n#             state = full_zero.clone()\n#             for i in range(self.num_qubits):\n#                 theta = sample[i]\n#                 ry = torch.tensor([\n#                     [torch.cos(theta / 2), -torch.sin(theta / 2)],\n#                     [torch.sin(theta / 2), torch.cos(theta / 2)]\n#                 ], dtype=torch.float32, device=device)\n\n#                 # Apply Ry gate\n#                 eye_prefix = torch.eye(2 ** i, device=device)\n#                 eye_suffix = torch.eye(2 ** (self.num_qubits - i - 1), device=device)\n#                 U = torch.kron(torch.kron(eye_prefix, ry), eye_suffix)\n#                 state = torch.matmul(U, state)\n#             state_batch.append(state)\n\n#         state_batch = torch.stack(state_batch)  # (batch_size, 2^n)\n\n#         # Compute âŸ¨Ïˆ|P|ÏˆâŸ© for each Pauli observable\n#         expectations = []\n#         for pauli in self.paulis:\n#             expvals = torch.einsum('bi,ij,bj->b', state_batch.conj(), pauli, state_batch)\n#             expectations.append(expvals.real)\n\n#         return torch.stack(expectations, dim=1)  # (batch_size, 8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.840849Z","iopub.execute_input":"2025-04-23T12:58:30.841143Z","iopub.status.idle":"2025-04-23T12:58:30.860433Z","shell.execute_reply.started":"2025-04-23T12:58:30.841122Z","shell.execute_reply":"2025-04-23T12:58:30.859616Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"\n\n# class HybridNN(nn.Module):\n#     def __init__(self, input_dim, num_classes, n_qubits=8):\n#         super().__init__()\n#         self.n_qubits = n_qubits\n#         self.scale = nn.Parameter(torch.ones(n_qubits) * torch.pi)\n\n#         # Classical frontâ€‘end\n#         self.classical = nn.Sequential(\n#             nn.Linear(input_dim, 64),\n#             nn.ReLU(),\n#             nn.Linear(64, n_qubits)          # â†’ (B, 8)\n#         )\n\n#         # Lightningâ€‘GPU device\n#         self.dev = qml.device(\"lightning.qubit\",\n#                               wires=n_qubits,\n#                               shots=None,        # analytic expectation\n#                               batch_obs=True)\n\n#         # Quantum layer wrapped as Torchâ€‘callable\n#         self.qnode = qml.QNode(self._circuit,\n#                                self.dev,\n#                                interface=\"torch\",\n#                                diff_method=\"adjoint\")\n\n#         # Classical tail\n#         self.output_layer = nn.Sequential(\n#             nn.Linear(n_qubits, 16),\n#             nn.ReLU(),\n#             nn.Linear(16, num_classes)\n#         )\n\n#     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n#     def _circuit(self, data, scale):\n#         \"\"\"Singleâ€‘sample, 8â€‘qubit circuit.\n#         data : shape (8,)\n#         scale: shape (8,) â€“ trainable\n#         \"\"\"\n#         # Data reâ€‘scaling & embedding\n#         for i in range(self.n_qubits):\n#             qml.RY(scale[i] * data[i], wires=i)\n\n#         # Ring entanglement\n#         for i in range(self.n_qubits):\n#             qml.CNOT(wires=[i, (i + 1) % self.n_qubits])\n\n#         # Expectation values <Z_i>\n#         return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n#     # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n#     def forward(self, x):\n#         \"\"\"\n#         x: (B, input_dim) â€“ full batch.\n#         PennyLane 0.36+ lets us pass the whole batch at once by\n#         giving tensor `x_emb` of shape (B, 8) as the first arg.\n#         \"\"\"\n#         x_emb = self.classical(x)                # (B, 8)\n\n#         # Call qnode over the batch; PennyLane will vectorise internally\n#         z_expect = self.qnode(x_emb, self.scale) # (B, 8)\n\n#         logits = self.output_layer(z_expect)\n#         return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.861431Z","iopub.execute_input":"2025-04-23T12:58:30.862421Z","iopub.status.idle":"2025-04-23T12:58:30.882125Z","shell.execute_reply.started":"2025-04-23T12:58:30.862389Z","shell.execute_reply":"2025-04-23T12:58:30.881190Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# input_dim = X_tensor.shape[1]\n# NUM_CLASSES = len(label_mapping)\n\n\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# criterion = torch.nn.CrossEntropyLoss()\n# model = HybridNN(input_dim=input_dim, num_classes=NUM_CLASSES).to(device)\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.882970Z","iopub.execute_input":"2025-04-23T12:58:30.883227Z","iopub.status.idle":"2025-04-23T12:58:30.905258Z","shell.execute_reply.started":"2025-04-23T12:58:30.883207Z","shell.execute_reply":"2025-04-23T12:58:30.903912Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def train_model(loader):\n    model.train()\n    running_loss = 0.0\n\n    for x, y in tqdm(loader):                     # x: (B, input_dim)\n        x, y = x.to(device), y.to(device)\n\n        optimizer.zero_grad()\n        logits = model(x)                   # (B, NUM_CLASSES)\n        loss   = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * x.size(0)\n\n    return running_loss / len(loader.dataset)\n\n\n@torch.no_grad()\ndef evaluate_model(loader):\n    model.eval()\n    running_loss, all_logits, all_labels = 0.0, [], []\n\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        logits = model(x)\n        loss   = criterion(logits, y)\n\n        running_loss += loss.item() * x.size(0)\n        all_logits.append(F.softmax(logits, dim=1).cpu())   # probabilities\n        all_labels.append(y.cpu())\n\n    all_logits = torch.cat(all_logits)\n    all_labels = torch.cat(all_labels)\n\n    return (running_loss / len(loader.dataset),\n            all_logits,                 # (N, NUM_CLASSES)\n            all_labels)                 # (N,)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.906429Z","iopub.execute_input":"2025-04-23T12:58:30.906772Z","iopub.status.idle":"2025-04-23T12:58:30.924440Z","shell.execute_reply.started":"2025-04-23T12:58:30.906739Z","shell.execute_reply":"2025-04-23T12:58:30.923571Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Quantum-only run","metadata":{}},{"cell_type":"code","source":"\n\n# # â”€â”€â”€ training loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# num_epochs = 30\n# best_val_auc, epochs_since_improvement = 0.0, 0\n# best_model_path = \"best_model.pth\"\n\n# train_losses, val_losses, val_aucs, test_aucs = [], [], [], []\n\n# for epoch in range(1, num_epochs + 1):\n#     print(f\"\\nEpoch {epoch}/{num_epochs}\")\n#     train_loss = train_model(train_loader)\n#     val_loss,  val_probs,  val_labels  = evaluate_model(val_loader)\n#     test_loss, test_probs, test_labels = evaluate_model(test_loader)\n\n#     # AUC scores\n#     val_auc  = roc_auc_score(val_labels.numpy(),\n#                              val_probs.numpy(),\n#                              multi_class=\"ovr\",\n#                              average=\"macro\")\n#     test_auc = roc_auc_score(test_labels.numpy(),\n#                              test_probs.numpy(),\n#                              multi_class=\"ovr\",\n#                              average=\"macro\")\n\n#     # bookkeeping\n#     train_losses.append(train_loss)\n#     val_losses.append(val_loss)\n#     val_aucs.append(val_auc)\n#     test_aucs.append(test_auc)\n\n#     # always save epoch checkpoint\n#     torch.save(model.state_dict(), f\"model_epoch_{epoch:02d}.pth\")\n\n#     # bestâ€‘model logic\n#     if val_auc > best_val_auc:\n#         best_val_auc = val_auc\n#         torch.save(model.state_dict(), best_model_path)\n#         epochs_since_improvement = 0\n#         print(f\"âœ…  New best model (val AUCÂ {val_auc:.4f})\")\n#     else:\n#         epochs_since_improvement += 1\n#         print(f\"âš ï¸   No improvement for {epochs_since_improvement} epoch(s)\")\n\n#         if epochs_since_improvement >= 5:\n#             # revert & shrink LR\n#             model.load_state_dict(torch.load(best_model_path))\n#             for g in optimizer.param_groups:\n#                 g[\"lr\"] *= 0.5\n#             print(f\"ğŸ”  Reverted to best model, LR halved to {g['lr']:.6f}\")\n#             epochs_since_improvement = 0\n\n#     # quick summary\n#     print(f\"TrainÂ {train_loss:.4f} | ValÂ {val_loss:.4f} (AUCÂ {val_auc:.4f}) \"\n#           f\"| TestÂ {test_auc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.925468Z","iopub.execute_input":"2025-04-23T12:58:30.925810Z","iopub.status.idle":"2025-04-23T12:58:30.946878Z","shell.execute_reply.started":"2025-04-23T12:58:30.925778Z","shell.execute_reply":"2025-04-23T12:58:30.945651Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class CosineQuantumLayer(nn.Module):\n    \"\"\"Fast surrogate for the Ry+CNOT ring producing âŸ¨Z_iâŸ©.\"\"\"\n    def __init__(self, n_qubits):\n        super().__init__()\n        self.scale = nn.Parameter(torch.ones(n_qubits) * torch.pi)\n\n    def forward(self, data):            # data : (B, n_qubits)\n        return torch.cos(self.scale * data)   # (B, n_qubits)\nclass HybridSideBySideFastNN(nn.Module):\n    def __init__(self, input_dim, num_classes, n_qubits=8):\n        super().__init__()\n        self.classical = nn.Sequential(\n            nn.Linear(input_dim, 64), nn.ReLU(),\n            nn.Linear(64, 32),        nn.ReLU(),\n            nn.Linear(32, 16),        nn.ReLU()\n        )\n        self.q_compress = nn.Linear(16, n_qubits)     # 16 â†’ 8\n        self.q_layer    = CosineQuantumLayer(n_qubits)\n\n        self.output = nn.Sequential(\n            nn.Linear(n_qubits + 16, 16), nn.ReLU(),\n            nn.Linear(16, num_classes)\n        )\n\n    def forward(self, x):                          # x : (B, input_dim)\n        x_emb  = self.classical(x)                 # (B, 16)\n        q_in   = self.q_compress(x_emb)            # (B, 8)\n        z_exp  = self.q_layer(q_in)                # (B, 8)\n\n        features = torch.cat([x_emb, z_exp], dim=1)  # (B, 24)\n        return self.output(features)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.947904Z","iopub.execute_input":"2025-04-23T12:58:30.948559Z","iopub.status.idle":"2025-04-23T12:58:30.968948Z","shell.execute_reply.started":"2025-04-23T12:58:30.948532Z","shell.execute_reply":"2025-04-23T12:58:30.968015Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"input_dim = X_tensor.shape[1]\nNUM_CLASSES = len(label_mapping)\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ncriterion = torch.nn.CrossEntropyLoss()\nmodel = HybridSideBySideFastNN(input_dim=input_dim, num_classes=NUM_CLASSES).to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:30.969743Z","iopub.execute_input":"2025-04-23T12:58:30.970403Z","iopub.status.idle":"2025-04-23T12:58:30.999370Z","shell.execute_reply.started":"2025-04-23T12:58:30.970378Z","shell.execute_reply":"2025-04-23T12:58:30.998556Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    classification_report\n)\n\nnum_epochs = 30\nbest_val_auc, epochs_since_improvement = 0.0, 0\nbest_model_path = \"best_model.pth\"\n\ntrain_losses, val_losses, val_aucs, test_aucs = [], [], [], []\n\nfor epoch in range(1, num_epochs + 1):\n    print(f\"\\nEpoch {epoch}/{num_epochs}\")\n    train_loss = train_model(train_loader)\n    val_loss,  val_probs,  val_labels  = evaluate_model(val_loader)\n    test_loss, test_probs, test_labels = evaluate_model(test_loader)\n\n\n    # AUC scores\n    val_auc  = roc_auc_score(val_labels.numpy(),\n                             val_probs.numpy(),\n                             multi_class=\"ovr\",\n                             average=\"macro\")\n    test_auc = roc_auc_score(test_labels.numpy(),\n                             test_probs.numpy(),\n                             multi_class=\"ovr\",\n                             average=\"macro\")\n\n    # bookkeeping\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    val_aucs.append(val_auc)\n    test_aucs.append(test_auc)\n\n    # always save epoch checkpoint\n    torch.save(model.state_dict(), f\"model_epoch_{epoch:02d}.pth\")\n\n    # bestâ€‘model logic\n    if val_auc > best_val_auc:\n        best_val_auc = val_auc\n        torch.save(model.state_dict(), best_model_path)\n        epochs_since_improvement = 0\n        print(f\"âœ…  New best model (val AUCÂ {val_auc:.4f})\")\n    else:\n        epochs_since_improvement += 1\n        print(f\"âš ï¸   No improvement for {epochs_since_improvement} epoch(s)\")\n\n        if epochs_since_improvement >= 5:\n            # revert & shrink LR\n            model.load_state_dict(torch.load(best_model_path))\n            for g in optimizer.param_groups:\n                g[\"lr\"] *= 0.5\n            print(f\"ğŸ”  Reverted to best model, LR halved to {g['lr']:.6f}\")\n            epochs_since_improvement = 0\n\n    # quick summary\n    from sklearn.metrics import (\n        accuracy_score,\n        precision_score,\n        recall_score,\n        f1_score,\n        classification_report\n    )\n    \n    # Get predicted class labels\n    val_preds = val_probs.numpy().argmax(axis=1)\n    test_preds = test_probs.numpy().argmax(axis=1)\n    val_true  = val_labels.numpy()\n    test_true = test_labels.numpy()\n    \n    # --- Validation metrics ---\n    val_acc      = accuracy_score(val_true, val_preds)\n    val_prec     = precision_score(val_true, val_preds, average=\"macro\", zero_division=0)\n    val_rec      = recall_score(val_true, val_preds, average=\"macro\", zero_division=0)\n    val_f1       = f1_score(val_true, val_preds, average=\"macro\", zero_division=0)\n    \n    # --- Test metrics ---\n    test_acc     = accuracy_score(test_true, test_preds)\n    test_prec    = precision_score(test_true, test_preds, average=\"macro\", zero_division=0)\n    test_rec     = recall_score(test_true, test_preds, average=\"macro\", zero_division=0)\n    test_f1      = f1_score(test_true, test_preds, average=\"macro\", zero_division=0)\n    \n    # Print metrics summary\n    print(f\"Val Acc: {val_acc:.4f} | Prec: {val_prec:.4f} | Rec: {val_rec:.4f} | F1: {val_f1:.4f}\")\n    print(f\"Test Acc: {test_acc:.4f} | Prec: {test_prec:.4f} | Rec: {test_rec:.4f} | F1: {test_f1:.4f}\")\n    print(\"Test Classification Report:\")\n    print(classification_report(test_true, test_preds, digits=4, zero_division=0))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T12:58:31.000385Z","iopub.execute_input":"2025-04-23T12:58:31.000638Z","iopub.status.idle":"2025-04-23T13:15:15.218495Z","shell.execute_reply.started":"2025-04-23T12:58:31.000618Z","shell.execute_reply":"2025-04-23T13:15:15.217478Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"\nEpoch 1/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ae714feb3b4b339ba92d1505b1f209"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.8928)\nVal Acc: 0.9755 | Prec: 0.5588 | Rec: 0.5425 | F1: 0.5479\nTest Acc: 0.9755 | Prec: 0.5582 | Rec: 0.5395 | F1: 0.5457\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9832    0.9869    0.9851    340611\n           1     0.8581    0.9756    0.9131     23694\n           2     0.0000    0.0000    0.0000       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.8422    0.9804    0.9061      1176\n           6     0.9559    0.6390    0.7660       881\n           7     0.9918    0.9821    0.9869     19244\n           8     0.0000    0.0000    0.0000       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9551    0.8253    0.8855       876\n          11     0.8444    0.8618    0.8530       825\n          12     0.9941    0.8928    0.9407     34607\n          13     0.9480    0.9486    0.9483      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9755    424182\n   macro avg     0.5582    0.5395    0.5457    424182\nweighted avg     0.9751    0.9755    0.9748    424182\n\n\nEpoch 2/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c963948cac1943f582d838765952d9ad"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.8965)\nVal Acc: 0.9783 | Prec: 0.5866 | Rec: 0.5654 | F1: 0.5709\nTest Acc: 0.9785 | Prec: 0.5763 | Rec: 0.5638 | F1: 0.5686\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9824    0.9913    0.9868    340611\n           1     0.8962    0.9569    0.9255     23694\n           2     0.0000    0.0000    0.0000       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9957    0.9864    0.9910      1176\n           6     0.9438    0.8956    0.9190       881\n           7     0.9966    0.9791    0.9877     19244\n           8     0.0435    0.0036    0.0066       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9573    0.9224    0.9395       876\n          11     0.8563    0.8812    0.8686       825\n          12     0.9953    0.8913    0.9404     34607\n          13     0.9779    0.9486    0.9631      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9785    424182\n   macro avg     0.5763    0.5638    0.5686    424182\nweighted avg     0.9775    0.9785    0.9777    424182\n\n\nEpoch 3/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73e3ed1eb3474d0f9337ef5717e546db"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.9312)\nVal Acc: 0.9795 | Prec: 0.6231 | Rec: 0.5969 | F1: 0.6035\nTest Acc: 0.9799 | Prec: 0.6225 | Rec: 0.5960 | F1: 0.6024\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9869    0.9885    0.9877    340611\n           1     0.8826    0.9860    0.9314     23694\n           2     0.0000    0.0000    0.0000       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9770    0.9753    0.9762      1176\n           6     0.9248    0.9353    0.9300       881\n           7     0.9953    0.9836    0.9894     19244\n           8     0.8571    0.3669    0.5139       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9115    0.9521    0.9313       876\n          11     0.8475    0.8824    0.8646       825\n          12     0.9846    0.9083    0.9449     34607\n          13     0.9704    0.9621    0.9662      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9799    424182\n   macro avg     0.6225    0.5960    0.6024    424182\nweighted avg     0.9797    0.9799    0.9794    424182\n\n\nEpoch 4/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"657f509538ea44249bbc6abf99ceafd2"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.9523)\nVal Acc: 0.9821 | Prec: 0.6371 | Rec: 0.6023 | F1: 0.6096\nTest Acc: 0.9826 | Prec: 0.6349 | Rec: 0.6031 | F1: 0.6103\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9874    0.9912    0.9893    340611\n           1     0.9091    0.9564    0.9321     23694\n           2     0.0000    0.0000    0.0000       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9881    0.9881    0.9881      1176\n           6     0.9147    0.9864    0.9492       881\n           7     0.9977    0.9964    0.9971     19244\n           8     0.9444    0.3669    0.5285       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9544    0.9555    0.9549       876\n          11     0.8705    0.9127    0.8911       825\n          12     0.9849    0.9254    0.9542     34607\n          13     0.9718    0.9682    0.9700      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9826    424182\n   macro avg     0.6349    0.6031    0.6103    424182\nweighted avg     0.9820    0.9826    0.9821    424182\n\n\nEpoch 5/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55b9262942664f929c81d3d31d3d5df8"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.9629)\nVal Acc: 0.9855 | Prec: 0.6091 | Rec: 0.6247 | F1: 0.6164\nTest Acc: 0.9858 | Prec: 0.6092 | Rec: 0.6273 | F1: 0.6176\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9908    0.9918    0.9913    340611\n           1     0.9165    0.9951    0.9542     23694\n           2     0.0000    0.0000    0.0000       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9781    0.9864    0.9822      1176\n           6     0.9456    0.9659    0.9556       881\n           7     0.9978    0.9987    0.9983     19244\n           8     0.5223    0.6331    0.5724       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9507    0.9692    0.9599       876\n          11     0.8740    0.9758    0.9221       825\n          12     0.9922    0.9279    0.9590     34607\n          13     0.9705    0.9664    0.9684      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9858    424182\n   macro avg     0.6092    0.6273    0.6176    424182\nweighted avg     0.9854    0.9858    0.9854    424182\n\n\nEpoch 6/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d8e9e7e80774b61b8ab3286810d1db7"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 1 epoch(s)\nVal Acc: 0.9878 | Prec: 0.6849 | Rec: 0.5699 | F1: 0.5930\nTest Acc: 0.9880 | Prec: 0.7085 | Rec: 0.5702 | F1: 0.5907\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9901    0.9953    0.9927    340611\n           1     0.9704    0.9959    0.9830     23694\n           2     1.0000    0.0046    0.0091       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     1.0000    0.4932    0.6606      1176\n           6     0.9118    0.9852    0.9471       881\n           7     0.9991    0.9904    0.9947     19244\n           8     0.9700    0.3489    0.5132       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9589    0.9064    0.9319       876\n          11     0.8675    0.9285    0.8970       825\n          12     0.9791    0.9462    0.9624     34607\n          13     0.9800    0.9584    0.9691      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9880    424182\n   macro avg     0.7085    0.5702    0.5907    424182\nweighted avg     0.9877    0.9880    0.9872    424182\n\n\nEpoch 7/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"317c1912b8574bddb276bc199cc239de"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.9840)\nVal Acc: 0.9911 | Prec: 0.6407 | Rec: 0.6088 | F1: 0.6138\nTest Acc: 0.9912 | Prec: 0.6383 | Rec: 0.6111 | F1: 0.6145\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9926    0.9967    0.9947    340611\n           1     0.9893    0.9981    0.9937     23694\n           2     0.0000    0.0000    0.0000       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9620    0.9915    0.9765      1176\n           6     0.8707    0.9864    0.9250       881\n           7     0.9988    0.9982    0.9985     19244\n           8     0.9700    0.3489    0.5132       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9543    0.9532    0.9537       876\n          11     0.8732    0.9770    0.9222       825\n          12     0.9831    0.9457    0.9640     34607\n          13     0.9802    0.9706    0.9754      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9912    424182\n   macro avg     0.6383    0.6111    0.6145    424182\nweighted avg     0.9904    0.9912    0.9906    424182\n\n\nEpoch 8/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d85e514fc1904a1bbb64d16f8c89f86c"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 1 epoch(s)\nVal Acc: 0.9907 | Prec: 0.6452 | Rec: 0.6110 | F1: 0.6179\nTest Acc: 0.9905 | Prec: 0.6439 | Rec: 0.6123 | F1: 0.6181\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9957    0.9926    0.9942    340611\n           1     0.9924    0.9804    0.9864     23694\n           2     0.0000    0.0000    0.0000       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9974    0.9872    0.9923      1176\n           6     0.9368    0.9762    0.9561       881\n           7     0.9978    0.9986    0.9982     19244\n           8     0.9700    0.3489    0.5132       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9695    0.9795    0.9744       876\n          11     0.8756    0.9636    0.9175       825\n          12     0.9411    0.9892    0.9646     34607\n          13     0.9826    0.9676    0.9750      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9905    424182\n   macro avg     0.6439    0.6123    0.6181    424182\nweighted avg     0.9899    0.9905    0.9900    424182\n\n\nEpoch 9/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e16a799afdf5483cb29f94718f62dfb2"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 2 epoch(s)\nVal Acc: 0.9893 | Prec: 0.6727 | Rec: 0.6288 | F1: 0.6297\nTest Acc: 0.9891 | Prec: 0.6761 | Rec: 0.6328 | F1: 0.6311\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9905    0.9962    0.9933    340611\n           1     0.9658    0.9970    0.9811     23694\n           2     0.8947    0.0780    0.1435       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9915    0.9881    0.9898      1176\n           6     0.9484    0.9796    0.9637       881\n           7     0.9991    0.9927    0.9959     19244\n           8     0.5481    0.6151    0.5797       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9459    0.9783    0.9618       876\n          11     0.8795    0.9733    0.9241       825\n          12     0.9963    0.9261    0.9599     34607\n          13     0.9820    0.9670    0.9744      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9891    424182\n   macro avg     0.6761    0.6328    0.6311    424182\nweighted avg     0.9889    0.9891    0.9887    424182\n\n\nEpoch 10/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6db15f67f3c40898a41905abf4a13ae"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 3 epoch(s)\nVal Acc: 0.9821 | Prec: 0.6361 | Rec: 0.6062 | F1: 0.6102\nTest Acc: 0.9824 | Prec: 0.6356 | Rec: 0.6082 | F1: 0.6110\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9970    0.9813    0.9891    340611\n           1     0.8480    0.9897    0.9134     23694\n           2     0.0000    0.0000    0.0000       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9798    0.9923    0.9861      1176\n           6     0.9614    0.9614    0.9614       881\n           7     0.9989    0.9986    0.9987     19244\n           8     1.0000    0.3489    0.5173       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9388    0.9463    0.9426       876\n          11     0.8842    0.9442    0.9132       825\n          12     0.9450    0.9976    0.9706     34607\n          13     0.9807    0.9633    0.9719      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9824    424182\n   macro avg     0.6356    0.6082    0.6110    424182\nweighted avg     0.9832    0.9824    0.9823    424182\n\n\nEpoch 11/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb3cd8e793c642afb73ee597ff342d36"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 4 epoch(s)\nVal Acc: 0.9905 | Prec: 0.7138 | Rec: 0.6112 | F1: 0.6245\nTest Acc: 0.9906 | Prec: 0.7114 | Rec: 0.6139 | F1: 0.6271\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9914    0.9972    0.9943    340611\n           1     0.9883    0.9984    0.9933     23694\n           2     1.0000    0.0780    0.1447       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9974    0.9855    0.9914      1176\n           6     0.9387    0.9739    0.9560       881\n           7     0.9998    0.9890    0.9944     19244\n           8     0.9417    0.3489    0.5092       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9588    0.9555    0.9571       876\n          11     0.8880    0.9806    0.9320       825\n          12     0.9840    0.9389    0.9609     34607\n          13     0.9825    0.9627    0.9725      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9906    424182\n   macro avg     0.7114    0.6139    0.6271    424182\nweighted avg     0.9902    0.9906    0.9901    424182\n\n\nEpoch 12/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0250b0f9b8b347bea01acb6b34dbf5c9"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.9894)\nVal Acc: 0.9928 | Prec: 0.7018 | Rec: 0.6183 | F1: 0.6257\nTest Acc: 0.9929 | Prec: 0.6981 | Rec: 0.6187 | F1: 0.6241\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9980    0.9934    0.9957    340611\n           1     0.9907    0.9981    0.9944     23694\n           2     1.0000    0.0505    0.0961       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9677    0.9923    0.9798      1176\n           6     0.9547    0.9818    0.9681       881\n           7     0.9978    0.9987    0.9983     19244\n           8     0.8031    0.3669    0.5037       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9652    0.9498    0.9574       876\n          11     0.8719    0.9733    0.9198       825\n          12     0.9509    0.9979    0.9738     34607\n          13     0.9714    0.9774    0.9744      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9929    424182\n   macro avg     0.6981    0.6187    0.6241    424182\nweighted avg     0.9927    0.9929    0.9924    424182\n\n\nEpoch 13/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3f17fd2c7b04f7fb6b67bc4781a572f"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 1 epoch(s)\nVal Acc: 0.9913 | Prec: 0.6449 | Rec: 0.6043 | F1: 0.6142\nTest Acc: 0.9912 | Prec: 0.7074 | Rec: 0.6220 | F1: 0.6408\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9925    0.9969    0.9947    340611\n           1     0.9836    0.9977    0.9906     23694\n           2     0.0000    0.0000    0.0000       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9948    0.9711    0.9828      1176\n           6     0.9602    0.9591    0.9597       881\n           7     0.9994    0.9983    0.9988     19244\n           8     0.9238    0.3489    0.5065       278\n           9     1.0000    0.2500    0.4000         4\n          10     0.9655    0.9258    0.9452       876\n          11     0.8601    0.9539    0.9046       825\n          12     0.9864    0.9461    0.9658     34607\n          13     0.9447    0.9817    0.9628      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9912    424182\n   macro avg     0.7074    0.6220    0.6408    424182\nweighted avg     0.9904    0.9912    0.9907    424182\n\n\nEpoch 14/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13f0263a598c43d8afc4f042e8f82e5b"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.9900)\nVal Acc: 0.9902 | Prec: 0.6842 | Rec: 0.6492 | F1: 0.6511\nTest Acc: 0.9900 | Prec: 0.7016 | Rec: 0.6157 | F1: 0.6281\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9970    0.9909    0.9940    340611\n           1     0.9751    0.9969    0.9859     23694\n           2     0.6875    0.0505    0.0940       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9672    0.9039    0.9345      1176\n           6     0.9298    0.9773    0.9530       881\n           7     0.9954    0.9979    0.9967     19244\n           8     0.2000    0.0036    0.0071       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9635    0.9053    0.9335       876\n          11     0.8825    0.9467    0.9135       825\n          12     0.9388    0.9975    0.9673     34607\n          13     0.9869    0.9645    0.9756      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9900    424182\n   macro avg     0.7016    0.6157    0.6281    424182\nweighted avg     0.9894    0.9900    0.9894    424182\n\n\nEpoch 15/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82de720ec40649bda3d34e0989904c0e"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 1 epoch(s)\nVal Acc: 0.9934 | Prec: 0.6840 | Rec: 0.6908 | F1: 0.6872\nTest Acc: 0.9931 | Prec: 0.6815 | Rec: 0.6600 | F1: 0.6649\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9969    0.9948    0.9958    340611\n           1     0.9915    0.9829    0.9872     23694\n           2     0.0000    0.0000    0.0000       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9864    0.9838    0.9851      1176\n           6     0.9379    0.9773    0.9572       881\n           7     0.9988    0.9984    0.9986     19244\n           8     0.5349    0.5791    0.5561       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9626    0.9406    0.9515       876\n          11     0.8637    0.9758    0.9163       825\n          12     0.9649    0.9978    0.9811     34607\n          13     0.9851    0.9700    0.9775      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9931    424182\n   macro avg     0.6815    0.6600    0.6649    424182\nweighted avg     0.9924    0.9931    0.9927    424182\n\n\nEpoch 16/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c114d9f65b24c5290514ac415ea30c1"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.9974)\nVal Acc: 0.9920 | Prec: 0.7771 | Rec: 0.6827 | F1: 0.6945\nTest Acc: 0.9920 | Prec: 0.8464 | Rec: 0.6685 | F1: 0.7010\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9927    0.9976    0.9952    340611\n           1     0.9901    0.9955    0.9928     23694\n           2     1.0000    0.0780    0.1447       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9749    0.9915    0.9831      1176\n           6     0.9402    0.9807    0.9600       881\n           7     0.9996    0.9985    0.9991     19244\n           8     0.9608    0.3525    0.5158       278\n           9     1.0000    0.2500    0.4000         4\n          10     0.9717    0.9795    0.9756       876\n          11     0.8895    0.9855    0.9350       825\n          12     0.9878    0.9475    0.9672     34607\n          13     0.9894    0.9700    0.9796      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9920    424182\n   macro avg     0.8464    0.6685    0.7010    424182\nweighted avg     0.9917    0.9920    0.9915    424182\n\n\nEpoch 17/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe8e58ed01c74d4b8f84f32434d71b41"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 1 epoch(s)\nVal Acc: 0.9926 | Prec: 0.7645 | Rec: 0.6629 | F1: 0.6794\nTest Acc: 0.9925 | Prec: 0.7532 | Rec: 0.6490 | F1: 0.6679\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9975    0.9934    0.9955    340611\n           1     0.9877    0.9931    0.9904     23694\n           2     0.6471    0.0505    0.0936       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9846    0.9787    0.9817      1176\n           6     0.9290    0.9807    0.9542       881\n           7     0.9988    0.9985    0.9986     19244\n           8     0.9684    0.3309    0.4933       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9708    0.9486    0.9596       876\n          11     0.8843    0.9818    0.9305       825\n          12     0.9516    0.9975    0.9740     34607\n          13     0.9787    0.9817    0.9802      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9925    424182\n   macro avg     0.7532    0.6490    0.6679    424182\nweighted avg     0.9922    0.9925    0.9921    424182\n\n\nEpoch 18/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"320a715c27d04897b4d1d7762e0bdaa4"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 2 epoch(s)\nVal Acc: 0.9854 | Prec: 0.7559 | Rec: 0.6500 | F1: 0.6725\nTest Acc: 0.9853 | Prec: 0.8155 | Rec: 0.6498 | F1: 0.6856\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9850    0.9973    0.9911    340611\n           1     0.9862    0.8773    0.9286     23694\n           2     0.6875    0.0505    0.0940       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9898    0.9872    0.9885      1176\n           6     0.9632    0.9796    0.9713       881\n           7     0.9991    0.9985    0.9988     19244\n           8     0.9000    0.3561    0.5103       278\n           9     1.0000    0.2500    0.4000         4\n          10     0.8783    0.9475    0.9116       876\n          11     0.8677    0.8824    0.8750       825\n          12     0.9866    0.9523    0.9692     34607\n          13     0.9894    0.9682    0.9787      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9853    424182\n   macro avg     0.8155    0.6498    0.6856    424182\nweighted avg     0.9848    0.9853    0.9846    424182\n\n\nEpoch 19/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"298d5274ac5d4ae89cfcba770bfdb538"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 3 epoch(s)\nVal Acc: 0.9931 | Prec: 0.6762 | Rec: 0.6295 | F1: 0.6297\nTest Acc: 0.9930 | Prec: 0.6856 | Rec: 0.6337 | F1: 0.6306\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9975    0.9941    0.9958    340611\n           1     0.9849    0.9911    0.9880     23694\n           2     1.0000    0.0505    0.0961       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9956    0.9558    0.9753      1176\n           6     0.9610    0.9784    0.9696       881\n           7     0.9987    0.9982    0.9984     19244\n           8     0.5411    0.6151    0.5758       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9773    0.9817    0.9795       876\n          11     0.8860    0.9794    0.9303       825\n          12     0.9618    0.9980    0.9796     34607\n          13     0.9795    0.9633    0.9713      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9930    424182\n   macro avg     0.6856    0.6337    0.6306    424182\nweighted avg     0.9929    0.9930    0.9927    424182\n\n\nEpoch 20/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b6ad43a9cdf4d96a49567ccbc3f5d42"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 4 epoch(s)\nVal Acc: 0.9916 | Prec: 0.7746 | Rec: 0.6831 | F1: 0.6943\nTest Acc: 0.9917 | Prec: 0.7064 | Rec: 0.6190 | F1: 0.6303\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9939    0.9960    0.9949    340611\n           1     0.9843    0.9941    0.9892     23694\n           2     0.8947    0.0780    0.1435       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9949    0.9881    0.9915      1176\n           6     0.9590    0.9830    0.9709       881\n           7     0.9991    0.9982    0.9986     19244\n           8     0.9709    0.3597    0.5249       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9576    0.9795    0.9684       876\n          11     0.8799    0.9855    0.9297       825\n          12     0.9755    0.9611    0.9682     34607\n          13     0.9862    0.9627    0.9743      1635\n          14     0.0000    0.0000    0.0000         2\n\n    accuracy                         0.9917    424182\n   macro avg     0.7064    0.6190    0.6303    424182\nweighted avg     0.9913    0.9917    0.9912    424182\n\n\nEpoch 21/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d44f51ef0e4e4e5ca23dd502c4a46313"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 5 epoch(s)\nğŸ”  Reverted to best model, LR halved to 0.000500\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1857892331.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(best_model_path))\n","output_type":"stream"},{"name":"stdout","text":"Val Acc: 0.9873 | Prec: 0.6909 | Rec: 0.6385 | F1: 0.6453\nTest Acc: 0.9875 | Prec: 0.6954 | Rec: 0.6222 | F1: 0.6327\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9918    0.9929    0.9924    340611\n           1     0.9332    0.9960    0.9636     23694\n           2     0.7778    0.0642    0.1186       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9644    0.9906    0.9773      1176\n           6     0.9435    0.9671    0.9552       881\n           7     0.9990    0.9975    0.9982     19244\n           8     0.0000    0.0000    0.0000       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9824    0.9566    0.9693       876\n          11     0.8712    0.9758    0.9205       825\n          12     0.9849    0.9437    0.9638     34607\n          13     0.9823    0.9492    0.9655      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9875    424182\n   macro avg     0.6954    0.6222    0.6327    424182\nweighted avg     0.9867    0.9875    0.9869    424182\n\n\nEpoch 22/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab451f1dd65249559dc648b38db783aa"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 1 epoch(s)\nVal Acc: 0.9910 | Prec: 0.7352 | Rec: 0.6976 | F1: 0.6982\nTest Acc: 0.9908 | Prec: 0.8024 | Rec: 0.6849 | F1: 0.7048\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9910    0.9978    0.9944    340611\n           1     0.9862    0.9977    0.9919     23694\n           2     0.7391    0.0780    0.1411       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9957    0.9864    0.9910      1176\n           6     0.9536    0.9807    0.9670       881\n           7     0.9995    0.9984    0.9990     19244\n           8     0.5423    0.6223    0.5796       278\n           9     1.0000    0.2500    0.4000         4\n          10     0.9578    0.9840    0.9707       876\n          11     0.8848    0.9867    0.9330       825\n          12     0.9961    0.9269    0.9602     34607\n          13     0.9893    0.9645    0.9768      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9908    424182\n   macro avg     0.8024    0.6849    0.7048    424182\nweighted avg     0.9905    0.9908    0.9904    424182\n\n\nEpoch 23/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83d685a61e06474191f94ac906bdcf30"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.9979)\nVal Acc: 0.9912 | Prec: 0.7786 | Rec: 0.6810 | F1: 0.6952\nTest Acc: 0.9910 | Prec: 0.8480 | Rec: 0.6674 | F1: 0.7026\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9909    0.9982    0.9945    340611\n           1     0.9912    0.9963    0.9937     23694\n           2     1.0000    0.0872    0.1603       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9966    0.9872    0.9919      1176\n           6     0.9620    0.9762    0.9690       881\n           7     0.9993    0.9987    0.9990     19244\n           8     0.9528    0.3633    0.5260       278\n           9     1.0000    0.2500    0.4000         4\n          10     0.9569    0.9874    0.9719       876\n          11     0.8863    0.9733    0.9278       825\n          12     0.9920    0.9297    0.9598     34607\n          13     0.9924    0.9639    0.9780      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9910    424182\n   macro avg     0.8480    0.6674    0.7026    424182\nweighted avg     0.9908    0.9910    0.9905    424182\n\n\nEpoch 24/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f054e389324157b72336f033b427df"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.9985)\nVal Acc: 0.9940 | Prec: 0.7713 | Rec: 0.6874 | F1: 0.6991\nTest Acc: 0.9938 | Prec: 0.7769 | Rec: 0.6561 | F1: 0.6787\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9960    0.9965    0.9962    340611\n           1     0.9905    0.9985    0.9945     23694\n           2     0.9524    0.0917    0.1674       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9949    0.9898    0.9923      1176\n           6     0.9578    0.9784    0.9680       881\n           7     0.9995    0.9985    0.9990     19244\n           8     0.9444    0.3669    0.5285       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9653    0.9852    0.9751       876\n          11     0.8883    0.9830    0.9333       825\n          12     0.9761    0.9780    0.9770     34607\n          13     0.9888    0.9755    0.9821      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9938    424182\n   macro avg     0.7769    0.6561    0.6787    424182\nweighted avg     0.9934    0.9938    0.9933    424182\n\n\nEpoch 25/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bb585cc6103472fb4ca814ec33660bd"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 1 epoch(s)\nVal Acc: 0.9942 | Prec: 0.7837 | Rec: 0.7054 | F1: 0.7162\nTest Acc: 0.9940 | Prec: 0.7656 | Rec: 0.6589 | F1: 0.6806\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9981    0.9946    0.9963    340611\n           1     0.9839    0.9982    0.9910     23694\n           2     0.7879    0.1193    0.2072       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9966    0.9864    0.9915      1176\n           6     0.9589    0.9796    0.9691       881\n           7     0.9994    0.9990    0.9992     19244\n           8     0.9608    0.3525    0.5158       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9654    0.9874    0.9763       876\n          11     0.8866    0.9855    0.9334       825\n          12     0.9641    0.9987    0.9811     34607\n          13     0.9817    0.9823    0.9820      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9940    424182\n   macro avg     0.7656    0.6589    0.6806    424182\nweighted avg     0.9937    0.9940    0.9936    424182\n\n\nEpoch 26/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d93c0deba8b148e7a023c35102d530eb"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 2 epoch(s)\nVal Acc: 0.9946 | Prec: 0.7635 | Rec: 0.6855 | F1: 0.6969\nTest Acc: 0.9945 | Prec: 0.7877 | Rec: 0.6720 | F1: 0.6967\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9980    0.9953    0.9967    340611\n           1     0.9893    0.9989    0.9941     23694\n           2     0.7692    0.0917    0.1639       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9966    0.9906    0.9936      1176\n           6     0.9616    0.9671    0.9643       881\n           7     0.9995    0.9989    0.9992     19244\n           8     0.9510    0.3489    0.5105       278\n           9     0.3333    0.2500    0.2857         4\n          10     0.9752    0.9874    0.9813       876\n          11     0.8886    0.9770    0.9307       825\n          12     0.9668    0.9980    0.9821     34607\n          13     0.9870    0.9768    0.9819      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9945    424182\n   macro avg     0.7877    0.6720    0.6967    424182\nweighted avg     0.9942    0.9945    0.9941    424182\n\n\nEpoch 27/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51cc4f67c964493f95263d43906c3ae0"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 3 epoch(s)\nVal Acc: 0.9933 | Prec: 0.7656 | Rec: 0.6687 | F1: 0.6867\nTest Acc: 0.9931 | Prec: 0.8301 | Rec: 0.6702 | F1: 0.7020\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9978    0.9939    0.9958    340611\n           1     0.9915    0.9956    0.9936     23694\n           2     0.7391    0.0780    0.1411       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9897    0.9804    0.9850      1176\n           6     0.9535    0.9773    0.9652       881\n           7     0.9993    0.9988    0.9991     19244\n           8     0.9706    0.3561    0.5211       278\n           9     1.0000    0.2500    0.4000         4\n          10     0.9816    0.9726    0.9771       876\n          11     0.8854    0.9830    0.9316       825\n          12     0.9516    0.9987    0.9746     34607\n          13     0.9912    0.9688    0.9799      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9931    424182\n   macro avg     0.8301    0.6702    0.7020    424182\nweighted avg     0.9929    0.9931    0.9927    424182\n\n\nEpoch 28/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acb22035a19f48e39494b1b42032bfe2"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 4 epoch(s)\nVal Acc: 0.9931 | Prec: 0.8429 | Rec: 0.6875 | F1: 0.7018\nTest Acc: 0.9929 | Prec: 0.9137 | Rec: 0.6741 | F1: 0.7110\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9928    0.9986    0.9957    340611\n           1     0.9912    0.9989    0.9950     23694\n           2     1.0000    0.1147    0.2058       218\n           3     1.0000    0.0312    0.0606       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9848    0.9915    0.9881      1176\n           6     0.9653    0.9784    0.9718       881\n           7     0.9993    0.9950    0.9972     19244\n           8     0.9352    0.3633    0.5233       278\n           9     1.0000    0.2500    0.4000         4\n          10     0.9665    0.9874    0.9768       876\n          11     0.8914    0.9745    0.9311       825\n          12     0.9965    0.9481    0.9717     34607\n          13     0.9828    0.9798    0.9813      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9929    424182\n   macro avg     0.9137    0.6741    0.7110    424182\nweighted avg     0.9929    0.9929    0.9925    424182\n\n\nEpoch 29/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dff8835d1d6a4508a7da9ecd34444f00"}},"metadata":{}},{"name":"stdout","text":"âš ï¸   No improvement for 5 epoch(s)\nğŸ”  Reverted to best model, LR halved to 0.000250\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1857892331.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(best_model_path))\n","output_type":"stream"},{"name":"stdout","text":"Val Acc: 0.9947 | Prec: 0.7774 | Rec: 0.6896 | F1: 0.7012\nTest Acc: 0.9945 | Prec: 0.7807 | Rec: 0.6590 | F1: 0.6819\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9979    0.9954    0.9967    340611\n           1     0.9918    0.9967    0.9942     23694\n           2     1.0000    0.1193    0.2131       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9974    0.9923    0.9949      1176\n           6     0.9609    0.9762    0.9685       881\n           7     0.9997    0.9990    0.9994     19244\n           8     0.9515    0.3525    0.5144       278\n           9     0.0000    0.0000    0.0000         4\n          10     0.9720    0.9897    0.9808       876\n          11     0.8913    0.9842    0.9355       825\n          12     0.9659    0.9983    0.9818     34607\n          13     0.9823    0.9817    0.9820      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9945    424182\n   macro avg     0.7807    0.6590    0.6819    424182\nweighted avg     0.9943    0.9945    0.9941    424182\n\n\nEpoch 30/30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1934 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45d2017cf0034d58bf633daa8010821b"}},"metadata":{}},{"name":"stdout","text":"âœ…  New best model (val AUCÂ 0.9989)\nVal Acc: 0.9954 | Prec: 0.7708 | Rec: 0.6902 | F1: 0.7021\nTest Acc: 0.9953 | Prec: 0.8404 | Rec: 0.6767 | F1: 0.7095\nTest Classification Report:\n              precision    recall  f1-score   support\n\n           0     0.9972    0.9971    0.9971    340611\n           1     0.9923    0.9959    0.9941     23694\n           2     0.9286    0.1193    0.2114       218\n           3     0.0000    0.0000    0.0000       128\n           4     0.0000    0.0000    0.0000         3\n           5     0.9949    0.9898    0.9923      1176\n           6     0.9623    0.9841    0.9731       881\n           7     0.9997    0.9990    0.9993     19244\n           8     0.9035    0.3705    0.5255       278\n           9     1.0000    0.2500    0.4000         4\n          10     0.9718    0.9852    0.9785       876\n          11     0.8888    0.9879    0.9357       825\n          12     0.9812    0.9914    0.9863     34607\n          13     0.9852    0.9798    0.9825      1635\n          14     1.0000    0.5000    0.6667         2\n\n    accuracy                         0.9953    424182\n   macro avg     0.8404    0.6767    0.7095    424182\nweighted avg     0.9949    0.9953    0.9949    424182\n\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}